---
title: "Golden Rules for Reproducible Statistical Analyses"
author: "Amy Gimma and Thibaut Jombart"
date: "`r format(Sys.time(), '%A %d %B %Y')`"
output:
  html_document:
    code_folding: show
    highlight: pygments
    number_sections: yes
    theme: spacelab
    toc: yes
    toc_collapse: no
    toc_depth: 2
    toc_float: yes
    css: !expr here::here('css', 'style.css')
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      collapse = TRUE,
                      fig.width = 8,
                      fig.height = 6,
                      dpi = 150,
                      warning = FALSE,
                      message = FALSE)
```


```{r read_scripts, echo = FALSE}

## read scripts
path_to_scripts <- here::here("scripts")

scripts_files <- dir(path_to_scripts, pattern = ".R$",
                     full.names = TRUE)
load_dictionary <- here::here("scripts", "load_dictionary.R")
for (file in scripts_files) if (!file == load_dictionary) source(file)

```


<!-- ===================================================== -->
<!-- ===================================================== -->
<!-- ===================================================== -->
<!-- Rule 1 -->
# Organize your document logically {.tabset .tabset-fade .tabset-pills}

<!-- ===================================================== -->
## Outline

A clear structure is essential for readers to understand and navigate your
analysis document. We recommend the following structure:

* Data preparation
* Analysis of *xxx*
* Analysis of *yyy*
* ...
* Export outputs
* (Optionally) System information

Where the analyses of *xxx*, *yyy*, ... should have identical structures, if
possible.



<!-- ===================================================== -->
## Data preparation

We recommend the following structure:

* **Load packages needed**

* **Load the raw data**

* **Clean the raw data** 
    + *standardise data* (e.g. `linelist::clean_data`)
    + *convert dates* that need converting (e.g. `linelist::guess_dates`)
    + *fix typos* (e.g. `linelist::clean_variable_spelling`)

* **Add new variables** (e.g. using `mutate`)

* **Subset entries** (rows) of the data (e.g. using `filter`)

* **Define custom colors** (e.g. using `scale_fill_manual` or `scale_color_manual`)



<!-- ===================================================== -->
## Data analysis

Organise the work in a systematic way, using one sub-section per type of
analysis, and starting with general analyses before subsetted or stratified
ones.

The following workflow should apply to most cases:

* **Data manipulations and computations**: this part will derive numbers and metrics
  needed by the analysis
  
* **Graphics**: graphical display of the results; we recommend using *ggplot2* where
  possible

* **Table**: a table providing numbers matching the graphics, and useful summary
  statistics, e.g. means, medians, confidence intervals, etc.



<!-- ===================================================== -->
## Export outputs

As a rule, we recommend to export every table displayed in the document to a
`.xlsx` and/or `.rds` files, so that they can be used in further analyses in R
(`.rds` file) or using other software. (`.xlsx` file). We recommend `.xlsx` over
text-based formats (e.g. `.txt`, `.csv`) as it preserves variable types.

The following code exports all tables named in `to_report` to `xslx` files,
stored inside the folder `produced_xlsx`; replace `xlsx` with `rds` for R files,
but in this case, do not insert the links as `rds` files cannot be opened within
a web browser (they are compressed binary representations of the data):

```{r xlsx_exports, eval = FALSE}

## create the produced_xlsx folder if it does not exist
if (!dir.exists("produced_xlsx")) {
  dir.create("produced_xlsx")
}

## vector of names of tables to export
## (these need to be existing data.frames or tibbles)
to_export <- c("table_1",
               "table_xxx",
               "table_yyy",
               "table_zzz")

## export all files
for (e in to_export) {
  rio::export(get(e),
              file.path("produced_xlsx",
                        paste0(e, ".xlsx")))
}

```

The following code will create links in your document pointing to the exported
files:

```{r links, results = "asis", eval = FALSE}

## note: for this to work, use the options `results = "asis"` and
## `echo = FALSE` in the code chunk header
for (e in to_export) {
  txt <- sprintf("- [%s.xlsx](%s.xlsx)",
                 e,
                 file.path("produced_xlsx",
                           e))
  cat(txt, sep = "\n")
}

```



<!-- ===================================================== -->
## System information

This is optional, but can be useful for audit purposes, or for diagnosing issues
in the results generated. We recommend including the following:

* `Sys.info()`: basic system information

* `R.version`: version of **R**

* `sessionInfo()`: which packages are loaded, and which versions are they?

* `params`: this list will contain optional parameters passed at compilation
  time through the `params` argument of `compile_report` or `update_reports`
  




<!-- ===================================================== -->
<!-- ===================================================== -->
<!-- ===================================================== -->
<!-- Rule 2 -->
# Follow standard naming conventions {.tabset .tabset-fade .tabset-pills}

<!-- ===================================================== -->
## Outline

Consistent and predictable naming helps streamline writing and reviewing
code. We recommend using the following conventions, already used to a large
extent in package development:

* **use only**: lower case letters, numbers, and `_` as separator (only use `-` for dates)

* **never use**: special characters in file names, variable names or values such as
  `éÈçôï\/# %?!&:;,@*^` and blank spaces

* if you really need to, only **use special characters only when defining labels**
  for graphics or tables
  
* **for dates**: use the format `yyyy-mm-dd`, e.g. `2001-10-13` for the 13th October
  2001.

* **for encoding**: use `UTF-8` encoding whenever possible, as it guarantees
  that special characters will display correctly on different computers

Note that all of these points, except for the encoding, will be achieved by
using `linelist::clean_data` on your `data.frame` or `tibble`.
 


## Example1: clean *vs* messy

The following data is messy for many reasons, as it violates all the above:

```{r messy}

messy <- linelist::messy_data(n = 10) %>% select(1:5)
messy

```

A clean version of this would be:

```{r clean_data}

clean <- clean_data(messy)
clean

```


## Example 2: special characters in graphics

Here we show how we can make exceptions to the naming conventions when
displaying graphics, without actually altering the data. We use the Korean MERS
dataset from the [*outbreaks*](https://www.repidemicsconsortium.org/outbreaks)
package, and use French labels to justify the accents:

```{r special_characters_graphics}

## load and clean data
mers <- outbreaks::mers_korea_2015$linelist %>%
  as_tibble() %>%
  clean_data()

mers

## define scales for sex and outcome, tweak labels as appropriate
scale_sex <- scale_fill_manual(
    "Sexe",
    values = c(m = "navy", f = "salmon"),
    labels = c(m = "Homme", f = "Femme"))

scale_x_outcome <- scale_x_discrete(
    labels = c(alive = "Vivant",
               dead = "Décédé"))

## make the plot
ggplot(mers, aes(x = outcome, fill = sex)) +
  geom_bar(position = "dodge") +
  scale_sex +
  scale_x_outcome +
  labs(title = "Issue de la maladie par sexe",
       x = "",
       y = "Nombre de cas")

```

Here, we are using accents, spaces, and upper-case letters for the graphics, but
because we restrict these to labels used in `scales`, the data remain clean,
respecting naming conventions.






<!-- ===================================================== -->
<!-- ===================================================== -->
<!-- ===================================================== -->
<!-- Rule 3 -->
# Use descriptive naming {.tabset .tabset-fade .tabset-pills}

<!-- ===================================================== -->
## Outline

All files and **R** objects should be named in an explicit way. There is a
trade-off between the length of a variable and explicit naming, but clarity is
often, if not always preferrable. Some exceptions can be made, e.g. by
systematically calling `x` your data.


<!-- ===================================================== -->
## Example

Here is an example of poor naming:

```{r naming_example_bad}

i <- iris %>%
  clean_data()
tab <- i %>%
  count(species)
tab

tab_2 <- i %>%
  group_by(species) %>%
  summarise_all(mean)
tab_2


a <- ggplot(i, aes(x = petal_width)) +
  geom_density(aes(fill = species), alpha = .4)
a

b <- ggplot(i, aes(x = petal_length)) +
  geom_density(aes(fill = species), alpha = .4)
b

```

This code is bad because:

* `iris`, the name of the dataset, is uselessly renamed to `i`, with no
  indication that the data has been cleaned

* `tab` and `tab_2` suggest these are *tables*, but provides no indication as to
  what they contain

* `a` and `b` are plots, but the name does not indicate that they are, or what
  the plots are about

A better version would be:

```{r naming_example_good}

iris_clean <- iris %>%
  clean_data()

table_species_counts <- iris_clean %>%
  count(species)
table_species_counts

table_species_means <- iris_clean %>%
  group_by(species) %>%
  summarise_all(mean)
table_species_means

plot_petal_width <- ggplot(iris_clean, aes(x = petal_width)) +
  geom_density(aes(fill = species), alpha = .4)
plot_petal_width

plot_petal_length <- ggplot(iris_clean, aes(x = petal_length)) +
  geom_density(aes(fill = species), alpha = .4)
plot_petal_length

```




<!-- ===================================================== -->
<!-- ===================================================== -->
<!-- ===================================================== -->
<!-- Rule 4 -->
# Write simple, readable code {.tabset .tabset-fade .tabset-pills}

<!-- ===================================================== -->
## Outline

Writing simple code is essential for enabling others (and in fact, yourself) to
read and understand your code, identify possible issues, or propose
improvements. Good code should be simple to read and understand. Here are some
guidelines for producing simple, readable code:

* **write short lines** (ideally less than 80 characters); many short lines are
  much easier to read than a single, long line of code

* **use intermediate variables**, e.g. when subsetting data, finding indices of
  items to modify; define named functions rather than lambdas (i.e. anonymous,
  on-the-fly functions used e.g. in `lapply` or `mutate`)
  
* **start a new line after each piping operator `%>%`**

* **use indentation** to make the code readable

* **describe your code**: provide a plain text description of the code, using
  numbered steps, and then refer to these steps in the code using comments 
  
* **use `##` for comments**, rather than a single `#` which are indented
  differently on different code editors


<!-- ===================================================== -->
## Example: breaking code in small bits

In this example, we use the `iris` dataset to compute summary statistics keeping
only the `setosa` and `versicolor` species, for sepal measurements. First, some
nasty version:

```{r iris_nasty}

lapply(iris_clean[iris_clean$species%in%c("setosa","versicolor"),grep("sepal",names(iris_clean))],summary)

```

The task is simple, but the code already complicated as too many things are
nested within the same line. Also note the absence of spaces after `,` and
operators, which makes the code less readable. The same code can be rewritten
in a much clearer way, first using base R:

```{r iris_better}

## find rows to keep: species setosa and versicolor
rows_to_keep <- iris_clean$species %in% c("setosa", "versicolor")

## identify columns with 'sepal' in their name
sepal_columns <- grep("sepal", names(iris_clean))

## subset data to analyse
sepals_setosa_versicolor <- iris_clean[rows_to_keep, sepal_columns]

## get summaries 
lapply(sepals_setosa_versicolor, summary)

```

A variant using `dplyr` would be:

```{r iris_dplyr}

iris_clean %>%
  filter(species %in% c("setosa","versicolor")) %>%
  select(contains("sepal")) %>%
  lapply(summary)

```



<!-- ===================================================== -->
## Example: useless complexity

Here we show some examples of un-needed complexity:

```{r useless_complexity, eval = FALSE}

## task: finding female, confirmed cases in x which have incompatible
## admission / discharge dates
timing_errors <- x %>%
  filter(!((date_admission < date_discharge) %in% TRUE) & (case_type %in% "confirmed") & !is.na(sex) & sex == "female")

```

* **Issues in the code**: 
    + `%in% TRUE` is useless; it is only valid if the left-hand side operand is
      itself already a `logical`; the comparison of a `logical` to `TRUE` is
      itself, i.e. `a == TRUE` is the same as `a`, by definition
    + the `!` ("not") operator is not needed; rather than identify items that do
      not fulfill the wrong condition, it is simpler to identify those who
      fulfill the right one
    + several additional conditions (logical `AND`) are best separated by colons
      in `filter` rather than `&`
    + `%in%` is only usefull if the right-hand side operand has multiple values;
      otherwise, `==` suffices
    + `!is.na(sex) & sex == "female"`: the first `!is.na(sex)` is not needed as
      `filter` takes care of `NAs`
    + long lines are harder to read
	
```{r useless_complexity_fix, eval = FALSE}

## alternative version: finding female, confirmed cases in x which have
## incompatible admission / discharge dates
timing_errors <- x %>%
  filter(date_admission >= date_discharge,
         case_type == "confirmed",
         sex == "female")

```



<!-- ===================================================== -->
## Example: fixing typos / recoding variables

In this example, we recode values of a variable `sex` with different
mis-spellings, using different approaches, starting with a complicated one.

```{r recoding-bad}

values <- c("m", NA, "Male", "fem", "F", "female", "male", "femme")
x <- data.frame(sex = sample(values, 20, replace = TRUE))
x

## one-line version - ugly
x %>% mutate(sex_clean = ifelse(sex == "m", "male", ifelse(is.na(sex), "unknown", ifelse(sex == "Male", "male", ifelse(sex == "fem", "female", ifelse(sex == "F", "female", ifelse(sex == "female", "female", ifelse(sex == "femme", "female", ifelse(sex == "male", "male", NA_character_)))))))))

```

```{r recoding-bad-indent}

## variant with indentation - still bad
x %>% mutate(sex_clean = ifelse(sex == "m", "male",
                           ifelse(is.na(sex), "unknown",
                             ifelse(sex == "Male", "male",
                               ifelse(sex == "fem", "female",
                                 ifelse(sex == "F", "female",
                                   ifelse(sex == "female", "female",
                                     ifelse(sex == "femme", "female",
                                       ifelse(sex == "male", "male", NA_character_)))))))))

```

The code above works, but is utterly unreadable, even when indented
properly. This causes multiple problems:

* near **impossible to identify mistakes** as the code is very hard to digest

* **very hard to expand** the code to account for new typos

* code produced is very long, which means **doing this for multiple variables and
  typos will be a problem**
  
As a rule, nested `ifelse` statements should always be avoided. As a
replacement, one can use `case_when`:

```{r case_when}

x %>% mutate(sex_clean = case_when(
                 sex == "m" ~ "male",
                 sex == "Male" ~ "male",
                 sex == "fem" ~ "female",
                 sex == "F" ~ "female",
                 sex == "femme" ~ "female",
                 sex == "female" ~ "female",
                 sex == "male" ~ "male",
                 TRUE ~ NA_character_
             ))

```

This code is simpler and more readable, but can still be improved. Yet another
alternative is offered by `linelist::clean_data`, which can use cleaning rules
to fix typos or recode variables. The procedure includes:

1. standardising data using `clean_data` to remove capitalisation issues, which
   simplifies the cleaning rules (e.g. no need to change `Male` to `male`)

2. defining cleaning rules for a variable (ideally in a separate Excel
   spreadsheet); note that because `clean_variable_spelling` already
   standardises entries, all spelling will be in lower-case,
   
3. apply these cleaning rules to the variable using  `linelist::clean_spelling` 

Let us illustrate this:

```{r clean_variable_spelling}

## load and show cleaning rules
cleaning_rules_file <- here::here("data", "cleaning_rules.xlsx")
cleaning_rules <- rio::import(cleaning_rules_file)
cleaning_rules

## clean data and apply cleaning rules
x %>%
  clean_data() %>%
  mutate(sex_clean = clean_spelling(sex, wordlist = cleaning_rules))

```

In practice, cleaning rules can be passed to `clean_data` directly, in which
cases variables for which cleaning rules are defined will be automatically
cleaned:

```{r clean_data_with_rules}

## generate clean dataset
x_clean <- x %>%
  clean_data(wordlists = cleaning_rules)

## compare old `x` and `x_clean`
cbind(x, x_clean)

```





<!-- Rule 6 -->
# Code systematically {.tabset .tabset-fade .tabset-pills}

## Outline

Be consistent in functions in libraries used

* *use `linelist::clean_data`* with a data dictionary for cleaning dates,
column names and values, etc. Take the time to understand how to use this 
package and how it works.

* for example choose between `n()`, `tally()`, and `count()` when summarizing
grouped data



## Example

Let's look at some basic code to count cases by location, gender and outcomes on
the MERS dataset from the `outbreaks` package.

```{r inconsistent}

mers <- outbreaks::mers_korea_2015$linelist %>%
  as_tibble()

# 2x2 table 1: location / sex
mers %>%
  group_by(place_infect, sex) %>%
  summarise(n = n())

## 2x2 table 2: location / outcome
loc_out_tab <- count(mers, place_infect, outcome)
loc_out_tab

##################################
## 2x2 table 3: location / outcome
##################################
table_sex_outcome <- with(mers, table(sex, outcome)) %>% as.data.frame() %>% as_tibble
table_sex_outcome

```

The code is correct, but there are plenty of consistency issues here; to name a
few:

* using 3 different approaches to construct the 2x2 tables

* not using the piping operator ` %>% ` in table 2, but using it elsewhere

* annotations are not consistent

* not going back to a new line after the piping operators for table 3

* in table 3, not using parenthesis for `as_tibble`, but using them for
  `as.data.frame()`

* storing the 2nd and 3rd table as objects, but not for the 1st one

* using different naming conventions: table 2 is abbreviated and `tab` comes
  last, whilst table 3 is explicit and `table` comes first in the name
  
A more consistent version of the code would look like:

```{r consistent}

## 2x2 table 1: location / sex
table_location_sex <- mers %>%
  count(place_infect, sex)
table_location_sex

## 2x2 table 2: location / outcome
table_location_outcome <- mers %>%
  count(place_infect, outcome)
table_location_outcome

## 2x2 table 3: location / outcome
table_sex_outcome <- mers %>%
  count(sex, outcome)
table_sex_outcome

```




<!-- ===================================================== --> 
<!-- ===================================================== --> 
<!-- ===================================================== --> 
<!-- Rule 6 -->
# Show statistical variation / uncertainty {.tabset .tabset-fade .tabset-pills}

## Outline

Variation in the data as well as the uncertainty associated to point estimates
(e.g. mean, proportion, etc) always need to be accounted for when analysing
data. Lack of doing so often leads to over-interpreting patterns, or to seeing
trends where there are, in fact, none.

Some guidelines can be used to avoid such issues:

* try to always plot the raw data before using summary statistics

* never plot results of a linear model / smoother / trend curve alone, but add
  it to the data

* when comparing summary statistics (e.g. mean, proportions) across different
  time points or groups, use confidence intervals to show the uncertainty
  associated with the estimation of these statistic; if unsure whether a
  confidence interval is needed, go through the decision tree below



```

 -------------------------    |Yes|----- **Include Confidence Interval**
|                         |     |  
|     Might the value     |     |
| observed be different   |-----|
| with a larger sample?   |     |                 |Yes|--| **CI not required**
|                         |     |    -----------   |
 -------------------------    |No|--| You sure? |--|
                                     -----------   |
                                                  |No|-- **Ask a statistician!**
```

## Example: showing variation in the data

Let us make some toy data, and plot a trend **without showing dispersion** in
the data:

```{r uncertainty_bad, fig.width = 4, fig.height = 4, fig.pos = "center"}

set.seed(1)
x <- rep(1:50, each = 15)
y <- rnorm(length(x), mean = 1)
df <- data.frame(x, y)

ggplot(df, aes(x = x, y = y)) +
  theme_bw() +
  geom_smooth() +
  labs(title = "Is there a trend here?")

```

Looking at this graph, **we may think there is a downward trend**, and start
interpreting what it means etc. **In fact, this is purely artifactual**. Data
have been generated in such a way that there is no correlation between the two
variables. This would be obvious if we add the data, thereby representing the
variability in the data:

```{r uncertainty_good, fig.width = 4, fig.height = 4, fig.pos = "center"}

ggplot(df, aes(x = x, y = y)) +
  theme_bw() +
  geom_point() +
  geom_smooth() +
  labs(title = "No, there is not!")

```




<!-- ===================================================== --> 
<!-- ===================================================== --> 
<!-- ===================================================== --> 
<!-- Rule 7 -->
# Always use relative paths {.tabset .tabset-fade .tabset-pills}

<!-- ===================================================== --> 
## Outline

Absolute paths point to a specific locationn on one's computer. Most likely,
this path exists only on this computer, so that files and folders referred this
way will not exist on other computers.

The alternative is to use relative path, i.e. path defined respective to a given
location, typically the project folder. These paths will work on any computer
which possess a copy of the project folder. Relative path are best set using the
`here` library with R project files to establish a root directory.


<!-- ===================================================== --> 
## Example

```{r paths, eval = FALSE}

## Bad practice:
x <- read.csv("C:/user/john_doe/secret_project/data/data_raw.csv")

## Good practice, after opening R in `secret_project`:
path_to_data <- here::here("data", "data_raw.csv") 
x <- read.csv(path_to_data)

```



<!-- Rule 8 -->
# Version and archive documents {.tabset .tabset-fade .tabset-pills}

Use semantic versioning in a clearly organized, standardized file structure

```
.
+-- linelist_investigations
|   +-- css
|   +-- data
|   +-- report_outputs
|   +-- report_sources
|       +-- aaa_clean_linelist_2019-29-09.Rmd
|       +-- epicurve_2019-04-10.Rmd
|       +-- _archive
|           +-- epicurve_2019-23-09.Rmd
|           +-- epicurve_2019-12-09.Rmd

```


<!-- Rule 10 -->
# Prepare presentation-ready plots {.tabset .tabset-fade .tabset-pills}

**Plots should be ready to copy and paste into an official presentation**

* check labels for correct spelling, accents, and grammar 
* uniform colors for variables throughout a document
* text size large
* avoid overlapping labels or other features


<!-- Rule 11 -->
# Always have your code reviewed {.tabset .tabset-fade .tabset-pills}

Ensure that:

* Analysis is appropriate to setting and underlying data is well understood, 
local collaborators should be included and acknowledged
* statistical methodology is applied correctly
* R code is readable and correctly implements methodology 
* markdown and the report document are well organized and properly formatted

<!-- Rule 12 -->
# Keep data safe {.tabset .tabset-fade .tabset-pills}

* Respect the privacy of the individuals described in the data

* Never label data as anonomysed without complying with best practices of the
standards and methods of anonymyzation (and clear final product with a 
qualified statistician)
